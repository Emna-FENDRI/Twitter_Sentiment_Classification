{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c9ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf854e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "files_path = \"/Users/asfoury/EPFL-MA1/ML/ML_project_2/project2-dataset/twitter-datasets\"\n",
    "neg_tweets = open(files_path + \"/train_neg.txt\")\n",
    "pos_tweets = open(files_path + \"/train_pos.txt\")\n",
    "neg_tweets_list = neg_tweets.readlines()\n",
    "pos_tweets_list = pos_tweets.readlines()\n",
    "print(len(neg_tweets_list))\n",
    "print(len(pos_tweets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e3dc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '...', '..']\n"
     ]
    }
   ],
   "source": [
    "punc = list(string.punctuation)\n",
    "punc += ['...', '..']\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c5ddb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glad i dot have taks tomorrow ! ! #thankful #startho\n",
      "\n",
      "['glad', 'i', 'dot', 'have', 'taks', 'tomorrow', '!', '!', '#', 'thankful', '#', 'startho']\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(neg_tweets_list[1])\n",
    "print(word_tokenize(neg_tweets_list[1]))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_tweet(tweet):\n",
    "    cleaned = word_tokenize(tweet)\n",
    "    cleaned = [w for w in cleaned if not w in stop_words]\n",
    "    cleaned = [w for w in cleaned if not w in punc]\n",
    "    return cleaned\n",
    "\n",
    "def clean_tweets(tweets_list):\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweets_list:\n",
    "        cleaned = clean_tweet(tweet)\n",
    "        cleaned_tweets.append(cleaned)\n",
    "    return cleaned_tweets\n",
    "\n",
    "neg_cleaned = clean_tweets(neg_tweets_list)\n",
    "pos_cleaned = clean_tweets(pos_tweets_list)\n",
    "print(len(neg_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb23766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# create the training set\n",
    "tr_set = [(tweet, -1) for tweet in neg_cleaned]\n",
    "temp = [(tweet, 1) for tweet in pos_cleaned]\n",
    "tr_set += temp\n",
    "print(len(tr_set))\n",
    "random.shuffle(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f73e1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796974\n",
      "[('user', 128018), ('url', 43642), (\"'s\", 21181), ('``', 20457), (\"n't\", 19486), (\"'m\", 13671), ('rt', 12180), ('love', 11181), ('like', 9284), ('get', 8645), ('frame', 7981), ('lol', 7769), ('3', 7707), ('u', 7705), ('good', 7700)]\n"
     ]
    }
   ],
   "source": [
    "# list of all words appearing in the tr_set\n",
    "all_words = []\n",
    "for tweet in neg_cleaned:\n",
    "    for word in tweet:\n",
    "        all_words.append(word)\n",
    "for tweet in pos_cleaned:\n",
    "    for word in tweet:\n",
    "        all_words.append(word)\n",
    "print(len(all_words))\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380a6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]\n",
    "def find_features(tweet):\n",
    "    words = set(tweet)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(tweet), label) for (tweet, label) in tr_set]\n",
    "mid = len(featuresets) // 2\n",
    "training_set = featuresets[:mid]\n",
    "test_set = featuresets[mid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f497941",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd071401",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9b69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141822d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
